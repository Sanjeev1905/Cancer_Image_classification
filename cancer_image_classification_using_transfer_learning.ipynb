{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPdtUQ+gC/9LnAGUYzV67yS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"YXMm5xHfrF5T","executionInfo":{"status":"ok","timestamp":1724021535671,"user_tz":-330,"elapsed":725,"user":{"displayName":"Sanjeev Ahirwar","userId":"02962951730834381106"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","from glob import glob\n","import seaborn as sns\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","# from keras.utils.np_utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n","\n","from tensorflow.keras.applications import VGG16, ResNet50\n","from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"code","source":["# Load metadata\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2-JHpXhPrNyR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Specify the correct path as a string\n","metadata_path = '/content/drive/MyDrive/cancer_image_classification/HAM10000_metadata.csv'\n","\n","# Load the CSV file\n","skin_df = pd.read_csv(metadata_path)\n","\n","SIZE = 224  # Size for ResNet50 and VGG16\n"],"metadata":{"id":"v0daQiDXrN1Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encode labels as numeric values\n","label_encoder = LabelEncoder()\n","skin_df['label'] = label_encoder.fit_transform(skin_df['dx'])\n","print(f\"Classes: {list(label_encoder.classes_)}\")\n","print(skin_df.sample(10))"],"metadata":{"id":"zyuGqLM6rN4C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize data distribution\n","fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n","# Distribution of diagnosis types\n","ax1 = axes[0, 0]\n","skin_df['dx'].value_counts().plot(kind='bar', ax=ax1)\n","ax1.set_ylabel('Count')\n","ax1.set_title('Diagnosis Types')\n","# Distribution of sex\n","ax2 = axes[0, 1]\n","skin_df['sex'].value_counts().plot(kind='bar', ax=ax2)\n","ax2.set_ylabel('Count')\n","ax2.set_title('Sex Distribution')\n","\n","# Localization of lesions\n","ax3 = axes[1, 0]\n","skin_df['localization'].value_counts().plot(kind='bar', ax=ax3)\n","ax3.set_ylabel('Count')\n","ax3.set_title('Localization')\n","\n","# Age distribution\n","ax4 = axes[1, 1]\n","sns.histplot(skin_df['age'].dropna(), kde=True, color='red', ax=ax4)\n","ax4.set_title('Age Distribution')\n","\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"zZxnNDDZrN6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Balance the dataset\n","class_counts = skin_df['label'].value_counts()\n","print(class_counts)\n","\n","# Resample each class to balance the dataset\n","n_samples = 500\n","balanced_dfs = [resample(skin_df[skin_df['label'] == i], replace=True, n_samples=n_samples, random_state=42) for i in range(7)]\n","skin_df_balanced = pd.concat(balanced_dfs)\n","\n","# Check the new distribution\n","print(skin_df_balanced['label'].value_counts())\n"],"metadata":{"id":"kcM4zbjprN9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_image(path):\n","    if path is None:\n","        print(f\"Warning: Path is None, skipping image.\")\n","        return None\n","    try:\n","        image = Image.open(path)\n","        image = image.resize((SIZE, SIZE))\n","        return np.asarray(image)\n","    except Exception as e:\n","        print(f\"Error loading image: {path}, Error: {str(e)}\")\n","        return None\n","\n","# Create a dictionary mapping image IDs to their paths\n","image_paths = {\n","    os.path.splitext(os.path.basename(x))[0]: x\n","    for x in glob(os.path.join('/content/drive/MyDrive/cancer_image_classification/cancer_type/HAM10000/', '*.jpg'))\n","}\n","\n","# Map the paths and load images\n","skin_df_balanced['path'] = skin_df_balanced['image_id'].map(image_paths.get)\n","\n","# Load and resize images, with error handling for missing paths\n","skin_df_balanced['image'] = skin_df_balanced['path'].map(load_image)\n"],"metadata":{"id":"eBQCZ1EgrN_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","# Ensure there are no None values in the image column\n","if skin_df_balanced['image'].isnull().any():\n","    raise ValueError(\"There are missing images in the dataset!\")\n","\n","# Convert the list of image arrays to a 4D NumPy array and normalize pixel values\n","X = np.array(skin_df_balanced['image'].tolist()) / 255.0\n","\n","# Extract labels and convert to one-hot encoding\n","Y = skin_df_balanced['label'].astype(int)  # Ensure labels are integers\n","Y_cat = to_categorical(Y, num_classes=7)  # Adjust num_classes based on your dataset\n","\n","# Check the shape of X and Y_cat to ensure correctness\n","print(f\"Shape of X: {X.shape}\")\n","print(f\"Shape of Y_cat: {Y_cat.shape}\")\n"],"metadata":{"id":"gcDy-rlDrdgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Specify the correct path as a string\n","metadata_path = '/content/drive/MyDrive/cancer_image_classification/HAM10000_metadata.csv'\n","\n","# Load the CSV file\n","skin_df = pd.read_csv(metadata_path)\n","\n","SIZE = 224  # Size for ResNet50 and VGG16\n"],"metadata":{"id":"GN6sal4qrdja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the dataset into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(X, Y_cat, test_size=0.25, random_state=42)\n"],"metadata":{"id":"Q7LL9NderdmC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data augmentation and preprocessing for both models\n","train_datagen_vgg = ImageDataGenerator(\n","    preprocessing_function=vgg_preprocess,\n","    validation_split=0.25\n",")\n","\n","validation_datagen_vgg = ImageDataGenerator(\n","    preprocessing_function=vgg_preprocess,\n","    validation_split=0.25\n",")\n","\n","train_datagen_resnet = ImageDataGenerator(\n","    preprocessing_function=resnet_preprocess,\n","    validation_split=0.25\n",")\n","\n","validation_datagen_resnet = ImageDataGenerator(\n","    preprocessing_function=resnet_preprocess,\n","    validation_split=0.25\n",")\n","\n","train_generator_vgg = train_datagen_vgg.flow(\n","    x_train, y_train,\n","    subset='training'\n",")\n","\n","validation_generator_vgg = validation_datagen_vgg.flow(\n","    x_test, y_test,\n","    subset='validation'\n",")\n","\n","train_generator_resnet = train_datagen_resnet.flow(\n","    x_train, y_train,\n","    subset='training'\n",")\n","\n","validation_generator_resnet = validation_datagen_resnet.flow(\n","    x_test, y_test,\n","    subset='validation'\n",")"],"metadata":{"id":"88SqGkUNrdo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define early stopping callback\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"],"metadata":{"id":"87sGzNB6rdrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define and compile VGG16 model\n","base_model_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n","for layer in base_model_vgg.layers:\n","    layer.trainable = False\n","\n","x_vgg = base_model_vgg.output\n","x_vgg = Flatten()(x_vgg)\n","x_vgg = Dense(512, activation='relu')(x_vgg)\n","x_vgg = Dropout(0.5)(x_vgg)\n","predictions_vgg = Dense(7, activation='softmax')(x_vgg)\n","\n","model_vgg = Model(inputs=base_model_vgg.input, outputs=predictions_vgg)\n","model_vgg.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"bkz8e1I6rdt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Train VGG16 model\n","history_vgg = model_vgg.fit(\n","    train_generator_vgg,\n","    epochs=20,\n","    validation_data=validation_generator_vgg,\n","    callbacks=[early_stopping]\n",")"],"metadata":{"id":"pzZGVSlLrdw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define and compile ResNet18 model\n","base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(SIZE, SIZE, 3))\n","for layer in base_model_resnet.layers:\n","    layer.trainable = False\n","\n","x_resnet = base_model_resnet.output\n","x_resnet = Flatten()(x_resnet)\n","x_resnet = Dense(512, activation='relu')(x_resnet)\n","x_resnet = Dropout(0.5)(x_resnet)\n","predictions_resnet = Dense(7, activation='softmax')(x_resnet)\n","\n","model_resnet = Model(inputs=base_model_resnet.input, outputs=predictions_resnet)\n","model_resnet.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n"],"metadata":{"id":"vrwlXAZ4rOCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train ResNet18 model\n","history_resnet = model_resnet.fit(\n","    train_generator_resnet,\n","    epochs=20,\n","    validation_data=validation_generator_resnet,\n","    callbacks=[early_stopping]\n",")\n"],"metadata":{"id":"-MW_SzuHr1FS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate models\n","loss_vgg, accuracy_vgg = model_vgg.evaluate(validation_generator_vgg)\n","print(f\"VGG16 - Validation Loss: {loss_vgg}\")\n","print(f\"VGG16 - Validation Accuracy: {accuracy_vgg}\")\n","\n","loss_resnet, accuracy_resnet = model_resnet.evaluate(validation_generator_resnet)\n","print(f\"ResNet18 - Validation Loss: {loss_resnet}\")\n","print(f\"ResNet18 - Validation Accuracy: {accuracy_resnet}\")\n","\n","# Make predictions\n","predictions_vgg = model_vgg.predict(x_test)\n","predictions_resnet = model_resnet.predict(x_test)"],"metadata":{"id":"DT-23Y6Kr1I7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eXpTcEwvr1Ls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"O2SnXl3Xr1Os"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IYM55TT8r1RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0QQFmKRZr1Tj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i1Lg64whr1ZE"},"execution_count":null,"outputs":[]}]}